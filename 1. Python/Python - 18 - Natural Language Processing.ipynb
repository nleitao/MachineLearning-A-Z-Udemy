{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language processing\n",
    "## Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('./../0. DataSets/Restaurant_Reviews.tsv',delimiter='\\t',quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business case desta vez e um restaurante que tem reviews na coluna 1 e tem binario 0 se o cliente nao gostou e 1 se o cliente que deixou a review gostou\n",
    "Vamos treinar um bag of words model para tentar prever se o comentario foi positivo ou negativo (coluna 2 e para confirmarmos a nossa analise)\n",
    "Primeiro: temos que limpar a data. Limpar os comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow    loved this place '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Primeiro limpar so o primeiro comment para ver se estamos a ir no caminho certo\n",
    "\n",
    "review=re.sub('[^a-zA-Z]',\" \",dataset['Review'][0])  #retirar todos os numeros e sinais de pontuacao. no fundo tirar tudo o que nao for a-z\n",
    "review=review.lower()  #por tudo em minusculas\n",
    "\n",
    "review  #ver como esta a ficar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/xbuns/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wow', 'loved', 'place']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords #importamos a library nltk no inicio e agora estamos a fazer o download do chamado \"stopwords\". Que sao as palavras que nao adicionam muita informacao a uma frase como por exemplo is, the, a, this, an etc..\n",
    "#Vamos entao remover estas stopwords dos nossos comments. comecando pelo primeiro\n",
    "\n",
    "review=review.split() #Separar a frase em palavras\n",
    "review=[word for word in review if not word in set(stopwords.words('english'))]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow', 'love', 'place']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Agora vamos fazer stemming. que e apenas manter a raiz da palavra. por exemplo em vez de ter loves, loved.. vamos passar tudo para love\n",
    "#na aula fizemos o stemming dentro do for loop anterior, mas vou fazer de forma separada para nao ficar confuso\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "review=[ps.stem(word) for word in review]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow love place'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#por a lista de strings de volta a uma so string\n",
    "review=\" \".join(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok. agora sabemos que isto funciona para um comment. vamos fazer para todos\n",
    "corpus=[]\n",
    "for i in range(0,1000):\n",
    "    review=re.sub('[^a-zA-Z]',\" \",dataset['Review'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[word for word in review if not word in set(stopwords.words('english'))]\n",
    "    ps=PorterStemmer()\n",
    "    review=[ps.stem(word) for word in review]\n",
    "    review=\" \".join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow love place',\n",
       " 'crust good',\n",
       " 'tasti textur nasti',\n",
       " 'stop late may bank holiday rick steve recommend love',\n",
       " 'select menu great price']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bag of words model\n",
    "#basicamente correr todos os comentarios e por cda palavra diferente numa coluna\n",
    "#se um determinado comentario (lines) tiver essa palavra por um 1 nessa coluna correspondete a essa palavra\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=1500) #este parametro max_fetures limita as columas a neste caso 1500 ou seja so as 1500 palavras mais frequentes\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "y=dataset.iloc[:,1].values #segunda coluna do dataset onde um 1 era um comentario positivo e o zero era negativo\n",
    "\n",
    "#agora temos uma matrix X e um vector y podemos usar um modulo de classification. Os recommendados para natural language processing sao\n",
    "#Naive Bayes, Decision tree, random forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55, 42],\n",
       "       [12, 91]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#COPIA do naive bayes classification exercise\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "\n",
    "#feature scaling.. nao e necessario aqui porque e tudo 0 ou 1 ja\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc_X = StandardScaler()\n",
    "# X_train=sc_X.fit_transform(X_train)\n",
    "# X_test=sc_X.transform(X_test)\n",
    "\n",
    "#fitting Naive Bayes to the training set.\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier=GaussianNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "#Predicting the Test set results\n",
    "y_pred=classifier.predict(X_test)\n",
    "\n",
    "#Making the confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_true=y_test,y_pred=y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ou seja 55 previsoes positivas correctas e 91 previsoes negativas correctas\n",
    "#12 42 incorrectas\n",
    "\n",
    "(55+91)/(55+42+12+91) #accuracy.. previsoes correctas a dividir pelo total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
