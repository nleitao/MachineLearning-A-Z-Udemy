{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importa o csv para um pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Datasets: Udemy: https://www.superdatascience.com/machine-learning/\n",
    "dataset=pd.read_csv(\"Machine Learning A-Z Template Folder/Part 1 - Data Preprocessing/Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faz com que os displays (tanto do pandas como dos arrays tenham notacao decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.round()\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "float_formatter = lambda x: \"%.2f\" % x\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ver o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separa as independent variables/features\n",
    "#ou seja tira todas as linhas e todas as colunas EXCEPTO a ultima coluna do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dataset.iloc[:,:-1]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#converte o subset dataset numa matrix/array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faz o mesmo mas para as dependent variables, ou seja tira a ultima coluna e transfor\n",
    "y=dataset.iloc[:,-1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limpa a data: usa este imputer que aplica aos missing_values (neste caso escolhi NaN) uma strategia (neste caso escolhi media da coluna)\n",
    "#escolho os parametros do imputer\n",
    "#fito o imputer as colunas onde quero limpar a data\n",
    "#transformo as colunas a serem tratadas com este imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer=Imputer(missing_values='NaN',strategy='mean',axis=0)\n",
    "imputer=imputer.fit(X[:,1:3])\n",
    "X[:,1:3]=imputer.transform(X[:,1:3])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformar categorical variables em numeros\n",
    "#agora vou transformar as variaveis categoricas (categorias = azul,verde ... ou Japao China) = coisas que nao consigo quantificar em numeros (tipo o One-Hot Encoding)\n",
    "#Para cada categoria vou ter um vector com 1s onde estava essa categoria (por exemplo vector 0 representa France vou ter um 1 onde estava a Franca antes... cria vectores para cada categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "label_encoder_X=LabelEncoder()\n",
    "X[:,0]=label_encoder_X.fit_transform(X[:,0])\n",
    "\n",
    "#Este OneHotEconder parameter categorical_features meto la dentro o indice da coluna que quero OneHotEncode, neste caso e zero porque quero o Pais\n",
    "onehotencoder=OneHotEncoder(categorical_features=[0])\n",
    "X=onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "label_encoder_y=LabelEncoder()\n",
    "y=label_encoder_y.fit_transform(y)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data set em train/test subsets\n",
    "#dividir a data em test set e train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=0)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "#como tenho diferentes variaveis com diferentes magnitudes preciso de as normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X=StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF PART1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Template. Usando o que preciso e apagando o que nao preciso (feature scaling por exemplo)\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('/home/ubuntu/workspace/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/Salary_Data.csv')\n",
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "# # Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0)\n",
    "\n",
    "# # Feature Scaling\n",
    "# \"\"\"from sklearn.preprocessing import StandardScaler\n",
    "# sc_X = StandardScaler()\n",
    "# X_train = sc_X.fit_transform(X_train)\n",
    "# X_test = sc_X.transform(X_test)\n",
    "# sc_y = StandardScaler()\n",
    "# y_train = sc_y.fit_transform(y_train)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#cria um objecto regressao linear e faz um fit nesse objecto do nosso training set.\n",
    "#no fundo cria uma recta que minimiza as distancias entre essa recta e os pontos do traning set \n",
    "#(x=idade e y=salarios)\n",
    "\n",
    "regressor=LinearRegression()\n",
    "regressor.fit(X_train,y_train)    #este regressor tem so a informacao dos parametros/coeficientes da recta\n",
    "\n",
    "#predicting the test set results\n",
    "#pegando na linha (regressao linear) que acabei de fazer, ve qual Ã© o y correspondentes aos pontos x \n",
    "#do meu test_set\n",
    "y_pred=regressor.predict(X_test)\n",
    "\n",
    "#Visualising the Training set results\n",
    "plt.scatter(X_train, y_train, color='red')\n",
    "plt.plot(X_train,regressor.predict(X_train),color='blue')\n",
    "plt.title(\"Salary vs. Experience (Training set)\")\n",
    "plt.xlabel(\"Years of experience\")\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faz o grafico dos pontos xy do dataset teste, para vermos de forma visual se estao muito longe da regressao\n",
    "plt.scatter(X_test, y_test, color='red')\n",
    "\n",
    "#aqui nao interessa que pontos passamos para o regressor porque a recta do modelo vai ser smpr igual\n",
    "plt.plot(X_train,regressor.predict(X_train),color='blue')\n",
    "plt.title(\"Salary vs. Experience (Test set)\")\n",
    "plt.xlabel(\"Years of experience\")\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF PART 2\n",
    "## MULTIPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing Template. Usando o que preciso e apagando o que nao preciso (feature scaling por exemplo)\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('/home/ubuntu/workspace/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 5 - Multiple Linear Regression/50_Startups.csv')\n",
    "\n",
    "#dividir as variaveis independente das dependentes (y=aquilo que no fim eu quero prever, neste caso: profit)\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165349.2, 136897.8, 471784.1, 2],\n",
       "       [162597.7, 151377.59, 443898.53, 0],\n",
       "       [153441.51, 101145.55, 407934.54, 1],\n",
       "       [144372.41, 118671.85, 383199.62, 2],\n",
       "       [142107.34, 91391.77, 366168.42, 1]], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #como nas VI tenho palavras tenho que as tornar primeiro em numero, para isso vou dar um label a elas (por exemplo NY passa a ser 2, Florida passa a ser 1, California =0)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "label_encoder_X=LabelEncoder()\n",
    "X[:,3]=label_encoder_X.fit_transform(X[:,3])\n",
    "\n",
    "#mostra so cinco linhas, so para ver\n",
    "X[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00, 0.00, 1.00, 165349.20, 136897.80, 471784.10],\n",
       "       [1.00, 0.00, 0.00, 162597.70, 151377.59, 443898.53],\n",
       "       [0.00, 1.00, 0.00, 153441.51, 101145.55, 407934.54],\n",
       "       [0.00, 0.00, 1.00, 144372.41, 118671.85, 383199.62],\n",
       "       [0.00, 1.00, 0.00, 142107.34, 91391.77, 366168.42]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Este OneHotEconder parameter categorical_features meto la dentro o indice da coluna que quero OneHotEncode, neste caso e zero porque quero o Pais\n",
    "#agora que escolhi a coluna onde tinha labels vou onehotencoding, ou seja vou ter uma coluna por label diferente e com 1 ou 0 se esse label corresponde a esta nova coluna ou nao.\n",
    "#as colunas que apliquei o onehotencoder passam smpr para o inicio da tabela\n",
    "onehotencoder=OneHotEncoder(categorical_features=[3])\n",
    "X=onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "#alterar a forma como os numeros sao mostrados, as casas decimais/tirar notacao cientifica etc\n",
    "dataset.round()\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "float_formatter = lambda x: \"%.2f\" % x\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "\n",
    "#mostra so cinco linhas, so para ver\n",
    "X[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Avoid the dummy variable trap = dps de fazer o onehotencoding temos que usar n-1 variaveis\n",
    "X=X[:,1:]\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#Feature Scaling not required on multiple linear regression: the library will take care of that for us\n",
    "\n",
    "#Fitting multiple linear regression to the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor=LinearRegression()\n",
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103015.20, 132582.28, 132447.74, 71976.10, 178537.48, 116161.24,\n",
       "       67851.69, 98791.73, 113969.44, 167921.07])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#depois de ter feito a regressao, fazer a previsao. dps comparamos este y_pred com o y_test\n",
    "y_pred=regressor.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a optimal model using Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   169.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>1.34e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:27:06</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1074.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.013e+04</td> <td> 6884.820</td> <td>    7.281</td> <td> 0.000</td> <td> 3.62e+04</td> <td>  6.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  198.7888</td> <td> 3371.007</td> <td>    0.059</td> <td> 0.953</td> <td>-6595.030</td> <td> 6992.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -41.8870</td> <td> 3256.039</td> <td>   -0.013</td> <td> 0.990</td> <td>-6604.003</td> <td> 6520.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.369</td> <td> 0.000</td> <td>    0.712</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.517</td> <td> 0.608</td> <td>   -0.132</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.574</td> <td> 0.123</td> <td>   -0.008</td> <td>    0.062</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.782</td> <th>  Durbin-Watson:     </th> <td>   1.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.41e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.572</td> <th>  Cond. No.          </th> <td>1.45e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.45e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     169.9\n",
       "Date:                Sun, 08 Jul 2018   Prob (F-statistic):           1.34e-27\n",
       "Time:                        17:27:06   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1063.\n",
       "Df Residuals:                      44   BIC:                             1074.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n",
       "x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n",
       "x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n",
       "x3             0.8060      0.046     17.369      0.000       0.712       0.900\n",
       "x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n",
       "x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n",
       "==============================================================================\n",
       "Omnibus:                       14.782   Durbin-Watson:                   1.283\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n",
       "Skew:                          -0.948   Prob(JB):                     2.41e-05\n",
       "Kurtosis:                       5.572   Cond. No.                     1.45e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.45e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ou seja, inicialmente, vamos incluir todas as variaveis independentes e dps vamos remover uma a uma as que nao sao\n",
    "#statistical relevant.\n",
    "#para isso vamos comecar por incluir tb no modelo o coeficiente da constante\n",
    "\n",
    "#adiciona uma coluna de 1s ao inicio da matrix X\n",
    "import statsmodels.formula.api as sm\n",
    "X=np.append(arr=np.ones((50,1)).astype(int), values=X ,axis=1)\n",
    "\n",
    "#aplicar steps da backward propagations\n",
    "#step 1 = define a significance level for a variable to stay in the model. Vou escolher SL=0.05\n",
    "\n",
    "#step 2 = Fit the full model with all possible predictors/variables\n",
    "X_opt=X[:,[0,1,2,3,4,5]]    #ou seja, o X tinha 6 colunas, vou escolher as 6 colunas neste passo 2.\n",
    "regressor.OLS=sm.OLS(endog=y, exog=X_opt).fit()\n",
    "\n",
    "#step 3 = Consider the predictor/variable with the highest P-value. If P > SL (because the lower the P value the more significant is that variable to the model), go to step 4, otherwise it's done\n",
    "#chama a funcao summary ao regressor para ver estas estatisticas do modelo, por exemplo para ver o P value\n",
    "regressor.OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   217.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>8.49e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:27:06</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1061.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1070.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.011e+04</td> <td> 6647.870</td> <td>    7.537</td> <td> 0.000</td> <td> 3.67e+04</td> <td> 6.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  220.1585</td> <td> 2900.536</td> <td>    0.076</td> <td> 0.940</td> <td>-5621.821</td> <td> 6062.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.606</td> <td> 0.000</td> <td>    0.714</td> <td>    0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.523</td> <td> 0.604</td> <td>   -0.131</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.592</td> <td> 0.118</td> <td>   -0.007</td> <td>    0.061</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.758</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.53e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.563</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.946\n",
       "Method:                 Least Squares   F-statistic:                     217.2\n",
       "Date:                Sun, 08 Jul 2018   Prob (F-statistic):           8.49e-29\n",
       "Time:                        17:27:06   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1061.\n",
       "Df Residuals:                      45   BIC:                             1070.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04\n",
       "x1           220.1585   2900.536      0.076      0.940   -5621.821    6062.138\n",
       "x2             0.8060      0.046     17.606      0.000       0.714       0.898\n",
       "x3            -0.0270      0.052     -0.523      0.604      -0.131       0.077\n",
       "x4             0.0270      0.017      1.592      0.118      -0.007       0.061\n",
       "==============================================================================\n",
       "Omnibus:                       14.758   Durbin-Watson:                   1.282\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.172\n",
       "Skew:                          -0.948   Prob(JB):                     2.53e-05\n",
       "Kurtosis:                       5.563   Cond. No.                     1.40e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#como a variavel x2 tem um p value de 0.990 quer dizer que nao e estatisticamente relevante para este modelo e posso aplicar o:\n",
    "X_opt=X[:,[0,1,3,4,5]]\n",
    "\n",
    "#repete agora o step 3 e 4 ate ter todas as variaveis com um p level < SL\n",
    "regressor.OLS=sm.OLS(endog=y, exog=X_opt).fit()\n",
    "regressor.OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   296.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>4.53e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:27:06</td>     <th>  Log-Likelihood:    </th> <td> -525.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1066.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.012e+04</td> <td> 6572.353</td> <td>    7.626</td> <td> 0.000</td> <td> 3.69e+04</td> <td> 6.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8057</td> <td>    0.045</td> <td>   17.846</td> <td> 0.000</td> <td>    0.715</td> <td>    0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0268</td> <td>    0.051</td> <td>   -0.526</td> <td> 0.602</td> <td>   -0.130</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0272</td> <td>    0.016</td> <td>    1.655</td> <td> 0.105</td> <td>   -0.006</td> <td>    0.060</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.838</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.949</td> <th>  Prob(JB):          </th> <td>2.21e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.586</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.948\n",
       "Method:                 Least Squares   F-statistic:                     296.0\n",
       "Date:                Sun, 08 Jul 2018   Prob (F-statistic):           4.53e-30\n",
       "Time:                        17:27:06   Log-Likelihood:                -525.39\n",
       "No. Observations:                  50   AIC:                             1059.\n",
       "Df Residuals:                      46   BIC:                             1066.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\n",
       "x1             0.8057      0.045     17.846      0.000       0.715       0.897\n",
       "x2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\n",
       "x3             0.0272      0.016      1.655      0.105      -0.006       0.060\n",
       "==============================================================================\n",
       "Omnibus:                       14.838   Durbin-Watson:                   1.282\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\n",
       "Skew:                          -0.949   Prob(JB):                     2.21e-05\n",
       "Kurtosis:                       5.586   Cond. No.                     1.40e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#como a variavel x1 tem um p value de 0.990 quer dizer que nao e estatisticamente relevante para este modelo e posso aplicar o:\n",
    "X_opt=X[:,[0,3,4,5]]\n",
    "\n",
    "#repete agora o step 3 e 4 ate ter todas as variaveis com um p level < SL\n",
    "regressor.OLS=sm.OLS(endog=y, exog=X_opt).fit()\n",
    "regressor.OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   450.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>2.16e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:27:07</td>     <th>  Log-Likelihood:    </th> <td> -525.54</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1057.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.698e+04</td> <td> 2689.933</td> <td>   17.464</td> <td> 0.000</td> <td> 4.16e+04</td> <td> 5.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.7966</td> <td>    0.041</td> <td>   19.266</td> <td> 0.000</td> <td>    0.713</td> <td>    0.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0299</td> <td>    0.016</td> <td>    1.927</td> <td> 0.060</td> <td>   -0.001</td> <td>    0.061</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.677</td> <th>  Durbin-Watson:     </th> <td>   1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.939</td> <th>  Prob(JB):          </th> <td>2.54e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.575</td> <th>  Cond. No.          </th> <td>5.32e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.32e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.950\n",
       "Model:                            OLS   Adj. R-squared:                  0.948\n",
       "Method:                 Least Squares   F-statistic:                     450.8\n",
       "Date:                Sun, 08 Jul 2018   Prob (F-statistic):           2.16e-31\n",
       "Time:                        17:27:07   Log-Likelihood:                -525.54\n",
       "No. Observations:                  50   AIC:                             1057.\n",
       "Df Residuals:                      47   BIC:                             1063.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n",
       "x1             0.7966      0.041     19.266      0.000       0.713       0.880\n",
       "x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n",
       "==============================================================================\n",
       "Omnibus:                       14.677   Durbin-Watson:                   1.257\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n",
       "Skew:                          -0.939   Prob(JB):                     2.54e-05\n",
       "Kurtosis:                       5.575   Cond. No.                     5.32e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.32e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt=X[:,[0,3,5]]\n",
    "\n",
    "#repete agora o step 3 e 4 ate ter todas as variaveis com um p level < SL\n",
    "regressor.OLS=sm.OLS(endog=y, exog=X_opt).fit()\n",
    "regressor.OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   849.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>3.50e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:27:07</td>     <th>  Log-Likelihood:    </th> <td> -527.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.903e+04</td> <td> 2537.897</td> <td>   19.320</td> <td> 0.000</td> <td> 4.39e+04</td> <td> 5.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8543</td> <td>    0.029</td> <td>   29.151</td> <td> 0.000</td> <td>    0.795</td> <td>    0.913</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.727</td> <th>  Durbin-Watson:     </th> <td>   1.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  18.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.911</td> <th>  Prob(JB):          </th> <td>9.44e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.361</td> <th>  Cond. No.          </th> <td>1.65e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.65e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.947\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     849.8\n",
       "Date:                Sun, 08 Jul 2018   Prob (F-statistic):           3.50e-32\n",
       "Time:                        17:27:07   Log-Likelihood:                -527.44\n",
       "No. Observations:                  50   AIC:                             1059.\n",
       "Df Residuals:                      48   BIC:                             1063.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\n",
       "x1             0.8543      0.029     29.151      0.000       0.795       0.913\n",
       "==============================================================================\n",
       "Omnibus:                       13.727   Durbin-Watson:                   1.116\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\n",
       "Skew:                          -0.911   Prob(JB):                     9.44e-05\n",
       "Kurtosis:                       5.361   Cond. No.                     1.65e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.65e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt=X[:,[0,3]]\n",
    "\n",
    "#repete agora o step 3 e 4 ate ter todas as variaveis com um p level < SL\n",
    "regressor.OLS=sm.OLS(endog=y, exog=X_opt).fit()\n",
    "regressor.OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codigo dado para fazer esta backward elimination de forma automatica:\n",
    "#nao sei como fazer as previsoes dps disto...\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "def backwardElimination(x, sl):\n",
    "    numVars = len(x[0])\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        if maxVar > sl:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    x = np.delete(x, j, 1)\n",
    "    regressor_OLS.summary()\n",
    "    return x\n",
    " \n",
    "SL = 0.05\n",
    "X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n",
    "X_Modeled = backwardElimination(X_opt, SL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
